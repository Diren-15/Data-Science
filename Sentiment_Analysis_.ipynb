{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPUTAptKd6qAK/nSqSXvopA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Diren-15/Data-Science/blob/main/Sentiment_Analysis_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *Sentiment Analysis*"
      ],
      "metadata": {
        "id": "Dy5cilSLmCXf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vQlKxU4GlypM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abd06253-3d5c-4b63-867c-f449f10d4e05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I Love this movie !\"\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IeavFsTGrYmI",
        "outputId": "4db01941-9866-44bb-8280-1d4d0baed3c6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'Love', 'this', 'movie', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lowercase_tokens = [token.lower() for token in tokens]\n",
        "print(lowercase_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrxWFCPWv6NV",
        "outputId": "4499f5a1-fea1-464d-90b1-1e50f97e9ec1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'love', 'this', 'movie', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = set(stopwords.words('english'))\n",
        "filtered_tokens = [token for token in lowercase_tokens if token not in stopwords]\n",
        "print(filtered_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogzU_5O1Yf5I",
        "outputId": "c71f1f10-4950-4a3d-b235-42707abc6b4d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['love', 'movie', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "cleaned_tokens = [re.sub(r'\\w\\s', '', token) for token in filtered_tokens]\n",
        "print(cleaned_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6G-E_6fZ1yf",
        "outputId": "5b048d3d-8de2-488c-b509-d465aaac8a29"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['love', 'movie', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()\n",
        "stemmed_tokens = [stemmer.stem(token) for token in cleaned_tokens]\n",
        "print(stemmed_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MfK8jKW6a7OX",
        "outputId": "eacd9286-d014-436f-96ce-fb4587117599"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['love', 'movi', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
      ],
      "metadata": {
        "id": "J7SJKYavfuNR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\"I love this movie !\", \"The movie is great\", \"I don't like this movie\"]\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "print(vectorizer.get_feature_names_out())\n",
        "print(X.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYXHAhZXf9SQ",
        "outputId": "77c6b599-c19d-47a9-d035-9a99769c913b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['don' 'great' 'is' 'like' 'love' 'movie' 'the' 'this']\n",
            "[[0 0 0 0 1 1 0 1]\n",
            " [0 1 1 0 0 1 1 0]\n",
            " [1 0 0 1 0 1 0 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\"I love this movie !\", \"The movie is great\", \"I don't like this movie\"]\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "print(vectorizer.get_feature_names_out())\n",
        "print(X.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYS_rKmJhaDe",
        "outputId": "f5463e92-3ca5-4fae-bd3e-c397c5464fd1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['don' 'great' 'is' 'like' 'love' 'movie' 'the' 'this']\n",
            "[[0.         0.         0.         0.         0.72033345 0.42544054\n",
            "  0.         0.54783215]\n",
            " [0.         0.54645401 0.54645401 0.         0.         0.32274454\n",
            "  0.54645401 0.        ]\n",
            " [0.5844829  0.         0.         0.5844829  0.         0.34520502\n",
            "  0.         0.44451431]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset from csv\n",
        "data = pd.read_csv('data.csv')\n",
        "X = data['Sentence']\n",
        "Y = data['Sentiment']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2, random_state = 42)\n",
        "\n",
        "# Feature Extraction\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_features = vectorizer.fit_transform(X_train)\n",
        "X_test_features = vectorizer.transform(X_test)\n",
        "\n",
        "# Train the SVM Classifier\n",
        "clf = svm.SVC()\n",
        "clf.fit(X_train_features, Y_train)\n",
        "\n",
        "# Make predictions\n",
        "Y_pred = clf.predict(X_test_features)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(Y_test, Y_pred)\n",
        "print('Accuracy !',accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6e1j_W6mKJR",
        "outputId": "4bf85a13-f2c4-48bd-d189-808690cb10aa"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy ! 0.6920444824636441\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Split the dataset into features (X) and labels (Y)\n",
        "sentences = data['Sentence'].values\n",
        "labels = data['Sentiment'].apply(lambda x: 1 if x == 'positive' else 0).values\n",
        "\n",
        "# Tokenize the sentences\n",
        "tokenizer = Tokenizer(num_words=5000, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n"
      ],
      "metadata": {
        "id": "Dmm4So0J8ZZB"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pad the sequences\n",
        "padded_sequences = pad_sequences(sequences, padding='post')\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Defiune the neural network architecture\n",
        "vocab_size = len(tokenizer.word_index) + 1 #added +1 because reserved 0 index for padding\n",
        "embedding_dim = 100 # you can choose any size for the embedding dim\n",
        "max_length = len(max(sequences, key=len))\n",
        "\n",
        "model = keras.Sequential()\n",
        "model.add(Embedding(input_dim = vocab_size, output_dim = embedding_dim, input_length = max_length))\n",
        "model.add(LSTM(units=128))\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "#Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, Y_train, epochs=10, batch_size=32)\n",
        "\n",
        "#Evaluate the model\n",
        "loss,accuracy = model.evaluate(X_test, Y_test)\n",
        "print(\"Loss :\", loss)\n",
        "print(\"Accuracy :\", accuracy)"
      ],
      "metadata": {
        "id": "JoQCCnHw-_Dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1adb3d29-884b-488d-a96b-ce6a71d8f7ca"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "147/147 [==============================] - 32s 186ms/step - loss: 0.6289 - accuracy: 0.6833\n",
            "Epoch 2/10\n",
            "147/147 [==============================] - 30s 201ms/step - loss: 0.6262 - accuracy: 0.6833\n",
            "Epoch 3/10\n",
            "147/147 [==============================] - 37s 252ms/step - loss: 0.6261 - accuracy: 0.6833\n",
            "Epoch 4/10\n",
            "147/147 [==============================] - 26s 178ms/step - loss: 0.6258 - accuracy: 0.6833\n",
            "Epoch 5/10\n",
            "147/147 [==============================] - 23s 155ms/step - loss: 0.6254 - accuracy: 0.6833\n",
            "Epoch 6/10\n",
            "147/147 [==============================] - 21s 142ms/step - loss: 0.6259 - accuracy: 0.6833\n",
            "Epoch 7/10\n",
            "147/147 [==============================] - 22s 152ms/step - loss: 0.6254 - accuracy: 0.6833\n",
            "Epoch 8/10\n",
            "147/147 [==============================] - 22s 151ms/step - loss: 0.6256 - accuracy: 0.6833\n",
            "Epoch 9/10\n",
            "147/147 [==============================] - 21s 145ms/step - loss: 0.6260 - accuracy: 0.6833\n",
            "Epoch 10/10\n",
            "147/147 [==============================] - 23s 155ms/step - loss: 0.6246 - accuracy: 0.6833\n",
            "37/37 [==============================] - 2s 38ms/step - loss: 0.6257 - accuracy: 0.6818\n",
            "Loss : 0.6257321834564209\n",
            "Accuracy : 0.6817793250083923\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "qVcJrzof6wdk"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the dataset from CSV\n",
        "data = pd.read_csv('data.csv')\n",
        "\n",
        "#Split the dataset into feature labels (X) and (Y)\n",
        "X = data['Sentence']\n",
        "Y = data['Sentiment']\n",
        "\n",
        "# Convert labels into numerical values\n",
        "encoder = LabelEncoder()\n",
        "Y = encoder.fit_transform(Y)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2, random_state = 42)\n",
        "\n",
        "# Feature Extraction\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_features = vectorizer.fit_transform(X_train)\n",
        "X_test_features = vectorizer.transform(X_test)\n",
        "\n",
        "# Train the model\n",
        "model = SVC()\n",
        "model.fit(X_train_features, Y_train)\n",
        "\n",
        "# Make Prediction\n",
        "Y_pred = model.predict(X_test_features)"
      ],
      "metadata": {
        "id": "wm1nfR148CL8"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "accuracy = accuracy_score(Y_test, Y_pred)\n",
        "precision = precision_score(Y_test,Y_pred, average = 'weighted')\n",
        "recall = recall_score(Y_test, Y_pred, average ='weighted')\n",
        "f1 = f1_score(Y_test, Y_pred, average = 'weighted')\n",
        "confusion_matrix = confusion_matrix(Y_test, Y_pred)\n",
        "\n",
        "print('Accuracy :', accuracy)\n",
        "print('Precision :', precision)\n",
        "print('Recall :', recall)\n",
        "print('F1 Score :', f1)\n",
        "print('Confusion Matrix :')\n",
        "print(confusion_matrix)\n",
        "\n",
        "# Hyperparameter tuning using GridSearchCV\n",
        "parameters = {'C' : [0.1,1,10], 'kernel' : ['linear','rbf']}\n",
        "grid_search = GridSearchCV(model, parameters, cv=5)\n",
        "grid_search.fit(X_train_features, Y_train)\n",
        "\n",
        "best_model = grid_search.best_estimator_\n",
        "best_params = grid_search.best_params_\n",
        "best_score = grid_search.best_score_\n",
        "\n",
        "print('Best Parameters :', best_params)\n",
        "print('Best Score:', best_score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Klj1MHgk-h5j",
        "outputId": "2a673a73-9463-498b-813d-da4403dc63e1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy : 0.6920444824636441\n",
            "Precision : 0.6579416519780992\n",
            "Recall : 0.6920444824636441\n",
            "F1 Score : 0.6506884603237336\n",
            "Confusion Matrix :\n",
            "[[ 14 115  46]\n",
            " [ 23 567  32]\n",
            " [  1 143 228]]\n",
            "Best Parameters : {'C': 0.1, 'kernel': 'linear'}\n",
            "Best Score: 0.6839242405157508\n"
          ]
        }
      ]
    }
  ]
}